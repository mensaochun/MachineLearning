# Decision tree

决策树要想到三个问题：1.哪一个作为头结点？2.连续特征怎么分？3.到什么时候分支结束？

## 一、信息论知识

### 1.熵的定义

![0](./pics/0.png)

> $H(p)中的p表示的是概率分布$

![1](./pics/1.png)

> 注意这个图只是表示随机变量只有两个取值的情况下的。多个取值实际上不能这么画出来。

### 2.条件熵

![2](./pics/2.png)

> 注意$H(Y/X)$中$Y,X$都是随机变量。这个不同于$H(Y/X=x_i)$。所以$H(Y/X)$实际上是X给定条件下Y的条件概率的熵对X的数学期望。

### 3.信息增益

![3](./pics/3.png)

![4](./pics/4.png)

![5](./pics/5.png)



![6](./pics/6.png)

![7](./pics/7.png)

> 注意:“信息增益比”不同于“信息增益和特征熵的比”。

### 4.基尼系数

> 自己的理解：当越不确定，纯度越低，这时候去算$\sum_i{p_i*p_i}$，它的值就会偏向于更小，而$1-\sum_i{p_i*p_i}$就更大。这个很类似于L2正则化，当参数分布得越均匀的时候，$\sum_i{w_i*w_i}$的值就会趋向于更小。
>
> 总之：gini越大，越不确定，纯度越低，对应特征越差。gini越小，越确定，纯度越大，对应特征越好。

![7](./pics/35.png)

![7](./pics/36.png)

![7](./pics/37.png)

## 二、ID3算法

![7](./pics/9.png)

![7](./pics/10.png)

![7](./pics/11.png)

## 三、C4.5算法

C4.5算法核心：

1. 对于ID3不能处理连续特征的问题，进行连续特征离散化。对于m个样本，将连续特征的取值按照从小到大进行排列，取相邻点的均值作为二元离散分类点。并且该特征在之后还可以进行分类。
2. ID3中的信息增益有一个问题，就是在相同的条件下，当该特征的取值越多的时候，信息增益更大，但是它们都是完全不确定的量。因此C4.5用信息增益比（信息增益/特征熵）来解决这个问题。
3. 特征值缺失问题的处理。

![7](./pics/12.png)

![7](./pics/13.png)

![7](./pics/14.png)

![7](./pics/15.png)

![7](./pics/16.png)

## 四、CART算法

CART的核心：

**对于分类而言**

1. ID3和C4.5都采用了熵模型，涉及大量的对数运算。因此CART使用基尼系数来代替。好处是使计算更为简单，计算量相对减小。
2. 使用二叉树结构，使得基尼系数的计算变得更加简单。
3. 对于连续特征和离散特征，都采用不断二分的方式。

**对于回归而言**

1. 采用均方差的形式来划分特征。
2. 预测的时候，用叶子节点的均值作为预测值。而对于分类，则是类别最多的类别作为预测值。

![7](./pics/17.png)

### 1.gini系数

![7](./pics/18.png)

![7](./pics/19.png)

![7](./pics/20.png)
![7](./pics/21.png)

### 2.分类树

![7](./pics/22.png)
![7](./pics/23.png)
![7](./pics/24.png)

### 3.回归树

![7](./pics/25.png)
![7](./pics/26.png)

### 4.剪枝

总结：

1.构建损失函数，损失函数包括训练误差和叶子节点数目。如果是分类，训练误差通过基尼系数来算，如果是回归，训练误差通过均方误差来算。都是针对叶子节点来说？

2.对于固定的alpha，一定存在使损失函数最小的子树。

![7](./pics/27.png)

> 注：$T$表示任意子树，$T_t$表示根节点为t的子树

剪枝的思路

![7](./pics/28.png)![7](./pics/29.png)

![7](./pics/30.png)
![7](./pics/31.png)

## 五、三种算法比较

![7](./pics/32.png)

## 六、总结

![7](./pics/33.png)
![7](./pics/34.png)


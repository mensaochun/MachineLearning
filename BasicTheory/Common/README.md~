---
typora-copy-images-to: pics
---

# 机器学习基础知识总结

## 机器学习和数据挖掘的区别

数据挖掘倾向于工程，机器学习属于理论。参考周志华的说法：

![9](./pics/9.png)

## 一、方差和偏差

### 1.自己的理解

首先要明白，方差和偏差指的是什么。

偏差指的是模型的预测值与真实值之间差的期望。

方差指的是模型的预测值的方差。

一个很容易让人困惑的就是，对于一个特定的输入，模型的预测值不是只有一个吗？如果这样，那哪里还有方差和偏差的说法？

实际上，模型的预测值不是只有一个，而是有多个模型，每个模型都有一个预测值，不同模型的预测值是不同的，这样模型的预测值实际上就是一个随机变量，因此就有偏差和方差的说法。

但是，为什么有多个模型呢？这是问题的关键。对于一个特定的假设，比如，$y=\theta_1x+\theta_0$，代表的是一个函数空间。这个假设实际上对应着多个模型，因为当模型参数$\theta_0,\theta_1$不同的时候，就对应着不同的模型。那么为什么有不同的模型参数呢？

这是因为存在不同的训练数据集。不同的数据会训练出不同的模型。这样就存在模型预测值的随机性。

### 2.什么是无偏估计和有偏估计

首先要明白的是，无偏估计讲的是什么对什么的估计是无偏的。举一个例子：样本均值是对总体均值的一个无偏估计，说的是样本均值与总体均值的。

接下来是一个来自于知乎的回答，讲得非常好了。

> 给你举个例子吧：
> 现在甲市有一万名小学三年级学生，他们进行了一次统考，考试成绩服从1~100的均匀分布：00001号学生得1分，00002号学生得1.01分……10000号学生得100分。那么他们的平均分是多少？(1+1.01+1.02+....+100)/10000=50.5，这个值叫做**总体平均数**。
>
> 现在假定你是教委的一个基层人员，教委主任给你一个早上时间，让你估算一下全市学生的平均成绩，你怎么办？把全市一万名学生都问一遍再计算时间显然是来不及了，因此在有限的时间里，你找到了一个聪明的办法：给全市的78所小学每一所学校打了一个电话，让他们随机选取一名学生的成绩报上来，这样你就得到了78个学生的成绩，这78个学生就是你的**样本**。
>
> 你现在的任务很简单了，拿这78个学生的成绩相加并除以78，你就得到了**样本平均数**。你把这个数报告给教委主任，这个数就是你估算出来的全市平均成绩。
>
> 这个样本平均数会不会等于总体平均数50.5？很显然这和你的“手气”有关——不过大多数情况下是不会相等的。
>
> 那么问题来了：既然样本平均数不等于总体平均数（也就是说你报给教委主任的平均分和实际的平均分非常有可能是不一样的），要它还有用吗？有！因为样本平均数是总体平均数的**无偏估计**——也就是说只要你采用这种方法进行估算，估算的结果的期望值（你可以近似理解为很多次估算结果的平均数）既不会大于真实的平均数，也不会小于之。换句话说：你这种估算方法**没有系统上的偏差**，而产生误差的原因只有一个：随机因素（也就是你的手气好坏造成的）。

总结一下就是：样本均值的期望如果等于总体均值，那么就是无偏估计。

另外，在李宏毅的课上也讲到了无偏估计和有偏估计。

![2](./pics\2.png)

假设现在要评估一个随机变量的均值$\mu$和方差$\sigma^2$。我们通过一次实验采样N个样本来计算样本均值$m$。那么，多次实验就可以得到多个样本均值。我们可以看到样本均值是分布在随机变量均值的周围的。计算样本均值$m$的期望，发现它就是等于随机变量均值$\mu$。因此它是一个无偏估计。

![3](./pics\3.png)

同样地，如果去做多次实验来计算样本方差$s^2$，我们发现$s^2$的期望是不等于$\sigma^2$的（也就是说，样本方差对总体方差的估计是有偏的），这跟每次实验的采样数N有关，当N越大，他们之间的差距也就越小。

### 3.李宏毅的讲解-误差来自于哪里？

参考李宏毅的ppt。

### 4.medium上的一遍文章

[Understanding the Bias-Variance Tradeoff](http://scott.fortmann-roe.com/docs/BiasVariance.html)

### 5.周志华关于方差和偏差的解释（这篇总结得最好）

![4](./pics/4.png)

![5](./pics/5.png)

![6](./pics/6.png)

![7](./pics/7.png)

![8](./pics/8.png)

## 二、评价指标

### 1.回归任务

均方误差

### 2.分类任务

**错误率**：分类错误的样本数占样本总数的比例。

**精度**（accuracy）：注意，我们平常喜欢说准确率，实际上是不对的。精度表示分类正确的样本数占样本总数的比例。

**准确率**（precision）：也叫查准率。说白一点，就是预测为某一类的所有样本中预测对的比例，即预测为某类你对了多少。

**召回率**（recall）：也叫查全率。说白一点，就是，我本来就是某一类的样本你预测对的比例，即某类样本你预测对了多少。

**漏报**：False Positive ：本来是正样例，分类成负样例，通常叫漏报。

**误报**：False Negative：本来是负样例，分类成正样例，通常叫误报。

### F1 score

precision和recall不能进行单独衡量性能的好坏，precision和recall是一对冤家，你大我就小，你小我就大，为了比较好地进行综合衡量，引入F1 score，对这两个东西进行调和平均：

![1535361868974](pics/1535361868974.png)

F1 score是0-1之间的。

**ROC**

ROC关注两个指标：

> **True  Positive Rate:**  TPR = TP / (TP+FN) → 将正例分对占所有正例的比例，召回
>  **Fales Positive Rate:**  FPR = FP / (FP+TN) → 将负例错分为正例占所有负例比例

在 ROC 空间中，每个点的横坐标是 FPR，纵坐标是 TPR，这也就描绘了分类器在 TP（真正率）和 FP（假正率）间的 trade-off。

![1535362283182](pics/1535362283182.png)

横轴越大，代表有越多的负例预测为了正例，这样正例的召回肯定高啊。

https://www.jianshu.com/p/b960305718f1

**AOU**

mAP

以上参考：周志华机器学习

另外，可以参考这篇：[机器学习评估指标的前世今生](https://zhuanlan.zhihu.com/p/36326966)